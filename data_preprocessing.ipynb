{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import librosa, librosa.display\n",
    "import scipy as sklearn\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav to CENS spectogram #\n",
    "import os\n",
    "\n",
    "path_from = '/Users/jonghochoi/Desktop/env1/Data/genres_original/'\n",
    "path_to = '/Users/jonghochoi/Desktop/env1/Data/images_cens/'\n",
    "\n",
    "hop_length = 512\n",
    "\n",
    "for genre in os.listdir(path_from):\n",
    "    if genre == '.DS_Store':\n",
    "        continue\n",
    "    for file in os.listdir(path_from + genre):\n",
    "        destination = path_to + genre + '/' + file[:-3] + 'jpg'\n",
    "        x, sr = librosa.load(path_from + genre + '/' + file)\n",
    "        chromagram = librosa.feature.chroma_cens(x, sr=sr, hop_length=hop_length)\n",
    "        librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(destination, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB to greyscale #\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "path_from = '/Users/jonghochoi/Desktop/env1/Data/images_cens/'\n",
    "path_to = '/Users/jonghochoi/Desktop/env1/Data/images_cens_greyscale/'\n",
    "\n",
    "for genre in os.listdir(path_from):\n",
    "    if genre == '.DS_Store':\n",
    "        continue\n",
    "    for file in os.listdir(path_from + genre):\n",
    "        if file == '.DS_Store':\n",
    "            continue\n",
    "        destination = path_to + genre + '/' + file\n",
    "        read_image = io.imread(path_from + genre + '/' + file)\n",
    "        img = color.rgb2gray(read_image)\n",
    "        io.imsave(destination,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_img = []\n",
    "y = []\n",
    "for category in os.listdir('/Users/jonghochoi/Desktop/env1/Data/images_cens/'):\n",
    "    if category != '.DS_Store':\n",
    "        for img_ in os.listdir('/Users/jonghochoi/Desktop/env1/Data/images_cens_greyscale/' + str(category)):\n",
    "            if img_ != '.DS_Store':\n",
    "                y.append(category)\n",
    "                img_path = '/Users/jonghochoi/Desktop/env1/Data/images_cens_greyscale/' + str(category) + '/' + str(img_)\n",
    "                img = imread(img_path, as_gray=True)\n",
    "                img = img.astype('float32')\n",
    "                img /= 255.0\n",
    "                train_img.append(img)\n",
    "\n",
    "x = np.array(train_img)\n",
    "\n",
    "labels = y\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(labels)\n",
    "targets = np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((799, 217, 334), (799,)), ((200, 217, 334), (200,)))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, targets, test_size=0.2)\n",
    "(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "x_train = x_train.reshape(799, 217, 334, 1)\n",
    "# x_train = torch.from_numpy(x_train)\n",
    "# y_train = torch.from_numpy(y_train)\n",
    "\n",
    "x_test = x_test.reshape(200, 217, 334, 1)\n",
    "# x_test = torch.from_numpy(x_test)\n",
    "# y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((799, 217, 334, 1), (799,)), ((200, 217, 334, 1), (200,)))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "x_test, y_test = shuffle(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_65 (Conv2D)          (None, 213, 330, 64)      1664      \n",
      "                                                                 \n",
      " max_pooling2d_62 (MaxPoolin  (None, 106, 165, 64)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 106, 165, 64)      0         \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 104, 163, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_63 (MaxPoolin  (None, 52, 81, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 52, 81, 128)       0         \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 51, 80, 256)       131328    \n",
      "                                                                 \n",
      " max_pooling2d_64 (MaxPoolin  (None, 25, 40, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 256000)            0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 512)               131072512 \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,353,280\n",
      "Trainable params: 131,353,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# CNN = models.Sequential()\n",
    "# CNN.add(layers.Conv2D(25, (100, 100), activation='relu', input_shape=(217,334,1)))\n",
    "\n",
    "# CNN.add(layers.AveragePooling2D())\n",
    "\n",
    "# CNN.add(layers.Dropout(0.2))\n",
    "\n",
    "# CNN.add(layers.Conv2D(16, (20, 20), activation='relu'))\n",
    "# CNN.add(layers.AveragePooling2D())\n",
    "\n",
    "# CNN.add(layers.Flatten())\n",
    "\n",
    "# CNN.add(layers.Dense(120, activation = 'relu'))\n",
    "\n",
    "# CNN.add(layers.Dense(84, activation = 'relu'))\n",
    "\n",
    "# CNN.add(layers.Dense(43, activation = 'softmax'))\n",
    "\n",
    "\n",
    "\n",
    "input_shape=(217,334,1)\n",
    "CNNmodel = models.Sequential()\n",
    "CNNmodel.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=input_shape))\n",
    "CNNmodel.add(layers.MaxPooling2D((2, 2)))\n",
    "CNNmodel.add(layers.Dropout(0.5))\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "CNNmodel.add(layers.MaxPooling2D((2, 2)))\n",
    "CNNmodel.add(layers.Dropout(0.5))\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(256, (2, 2), activation='relu'))\n",
    "CNNmodel.add(layers.MaxPooling2D(2,2))\n",
    "CNNmodel.add(layers.Flatten())\n",
    "\n",
    "# CNNmodel.add(layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "# CNNmodel.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# CNNmodel.add(layers.Dense(64, activation='relu'))\n",
    "# CNNmodel.add(layers.Dropout(0.5))\n",
    "\n",
    "# CNNmodel.add(layers.Conv2D(512, (2, 2), activation='relu'))\n",
    "# CNNmodel.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "CNNmodel.add(layers.Dense(512, activation='relu'))\n",
    "CNNmodel.add(layers.Dense(128, activation='relu'))\n",
    "CNNmodel.add(layers.Dense(64, activation='softmax'))\n",
    "CNNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 54s 7s/step - loss: 7.4074 - accuracy: 0.0839 - val_loss: 4.1246 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 52s 7s/step - loss: 3.2587 - accuracy: 0.0901 - val_loss: 3.6148 - val_accuracy: 0.1150\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 51s 6s/step - loss: 2.5020 - accuracy: 0.1064 - val_loss: 2.7311 - val_accuracy: 0.0750\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 50s 6s/step - loss: 2.3895 - accuracy: 0.1176 - val_loss: 3.1825 - val_accuracy: 0.0950\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 52s 6s/step - loss: 2.3098 - accuracy: 0.1289 - val_loss: 3.0588 - val_accuracy: 0.0750\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 51s 6s/step - loss: 2.2502 - accuracy: 0.1777 - val_loss: 2.5992 - val_accuracy: 0.0800\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 50s 6s/step - loss: 2.2335 - accuracy: 0.1677 - val_loss: 2.9187 - val_accuracy: 0.1800\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 51s 7s/step - loss: 2.1574 - accuracy: 0.2628 - val_loss: 2.8730 - val_accuracy: 0.1550\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 57s 7s/step - loss: 2.1099 - accuracy: 0.2478 - val_loss: 2.5578 - val_accuracy: 0.2250\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 1.9919 - accuracy: 0.3191 - val_loss: 2.2865 - val_accuracy: 0.2250\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 55s 7s/step - loss: 1.8554 - accuracy: 0.3542 - val_loss: 2.1514 - val_accuracy: 0.2600\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 56s 7s/step - loss: 1.7154 - accuracy: 0.4155 - val_loss: 2.1217 - val_accuracy: 0.2600\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 56s 7s/step - loss: 1.6200 - accuracy: 0.4443 - val_loss: 2.0932 - val_accuracy: 0.2500\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 56s 7s/step - loss: 1.4444 - accuracy: 0.5181 - val_loss: 2.1442 - val_accuracy: 0.2900\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 57s 7s/step - loss: 1.3664 - accuracy: 0.5369 - val_loss: 2.2471 - val_accuracy: 0.2300\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 57s 7s/step - loss: 1.2680 - accuracy: 0.5707 - val_loss: 2.2250 - val_accuracy: 0.2650\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 1.0718 - accuracy: 0.6395 - val_loss: 2.3901 - val_accuracy: 0.2400\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 59s 7s/step - loss: 0.8673 - accuracy: 0.6934 - val_loss: 2.7497 - val_accuracy: 0.2350\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 0.8131 - accuracy: 0.7171 - val_loss: 2.5040 - val_accuracy: 0.2750\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 0.6400 - accuracy: 0.7985 - val_loss: 2.9909 - val_accuracy: 0.2800\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 59s 7s/step - loss: 0.4272 - accuracy: 0.8686 - val_loss: 3.3219 - val_accuracy: 0.2700\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 0.3101 - accuracy: 0.8986 - val_loss: 3.6202 - val_accuracy: 0.2800\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 0.2322 - accuracy: 0.9312 - val_loss: 3.8797 - val_accuracy: 0.2900\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 59s 7s/step - loss: 0.1927 - accuracy: 0.9387 - val_loss: 4.0736 - val_accuracy: 0.2900\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 58s 7s/step - loss: 0.1464 - accuracy: 0.9637 - val_loss: 4.8178 - val_accuracy: 0.2850\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 60s 7s/step - loss: 0.1394 - accuracy: 0.9612 - val_loss: 4.6424 - val_accuracy: 0.2800\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.1031 - accuracy: 0.9750 - val_loss: 4.7685 - val_accuracy: 0.3000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0636 - accuracy: 0.9850 - val_loss: 5.1391 - val_accuracy: 0.3100\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.1366 - accuracy: 0.9700 - val_loss: 5.0516 - val_accuracy: 0.2850\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 59s 7s/step - loss: 0.1077 - accuracy: 0.9762 - val_loss: 4.5992 - val_accuracy: 0.2600\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 60s 7s/step - loss: 0.0877 - accuracy: 0.9737 - val_loss: 4.8257 - val_accuracy: 0.3050\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0791 - accuracy: 0.9737 - val_loss: 5.0700 - val_accuracy: 0.3050\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0512 - accuracy: 0.9887 - val_loss: 4.8667 - val_accuracy: 0.2850\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0517 - accuracy: 0.9800 - val_loss: 5.4232 - val_accuracy: 0.3050\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0403 - accuracy: 0.9862 - val_loss: 5.6046 - val_accuracy: 0.3150\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0282 - accuracy: 0.9950 - val_loss: 5.3396 - val_accuracy: 0.2800\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0188 - accuracy: 0.9987 - val_loss: 5.7651 - val_accuracy: 0.3100\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0147 - accuracy: 0.9975 - val_loss: 6.0239 - val_accuracy: 0.3000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0162 - accuracy: 0.9962 - val_loss: 6.4564 - val_accuracy: 0.2800\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0124 - accuracy: 0.9987 - val_loss: 6.2989 - val_accuracy: 0.2950\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 62s 8s/step - loss: 0.0090 - accuracy: 0.9987 - val_loss: 6.4360 - val_accuracy: 0.3000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 6.4487 - val_accuracy: 0.2900\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 64s 8s/step - loss: 0.0135 - accuracy: 0.9987 - val_loss: 6.4477 - val_accuracy: 0.3000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.0156 - accuracy: 0.9962 - val_loss: 6.4775 - val_accuracy: 0.3050\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 66s 8s/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 6.4532 - val_accuracy: 0.2850\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 68s 9s/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 6.8199 - val_accuracy: 0.2800\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 68s 9s/step - loss: 0.0133 - accuracy: 0.9987 - val_loss: 6.4780 - val_accuracy: 0.2950\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.0082 - accuracy: 0.9950 - val_loss: 6.4800 - val_accuracy: 0.3050\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 6.4489 - val_accuracy: 0.3050\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 6.7074 - val_accuracy: 0.2850\n"
     ]
    }
   ],
   "source": [
    "CNNmodel.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = CNNmodel.fit(x_train,\n",
    "                 y_train,\n",
    "                 batch_size = 100,\n",
    "                 epochs = 50,\n",
    "                 verbose = 1,\n",
    "                 validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "687c7bd9bfc30de0eb499b86a667cff972914d7d186d5367031e44cea3330a10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
